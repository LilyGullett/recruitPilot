title: "Accuracy Analysis"
author: "Lily Gullett -- lilysgullett@outlook.com"
output:
  html_document:
    theme: flatly
    code_folding: show
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_doctument:
      toc: yes
      toc_depth: 3
---
```{r, setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, fig.align = 'left')
knitr::opts_knit$set(root.dir = "../data")
options(width = 88)
library(magrittr)
```





### version: `r Sys.Date() %>% format(format="%B %d, %Y")`

<!-- this is where the DOI would go  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3675991.svg)](https://doi.org/10.5281/zenodo.3675991)
-->


#### [GitHub repository](https://github.com/LilyGullett/recruitPilot){target="_blank"}
###

***
This is the working analysis pipeline to analyze data generated from the recruit pilot study manual inventories and TagLab segmentation and assess differences in the inventory counts between trained and untrained eyes as well as between TagLab segmentation and manual inventory. 

***

### All analyses performed with R version `r getRversion()`


# Basic setup of R environment
***
## Loading required packages
For the following analyses we will require the use of a number of different R packages. Most of which can be sourced from CRAN, but some must be downloaded from GitHub. We can use the following code to load in the packages and install any packages not previously installed in the R console. 


```{r,packages, include = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
if (!require("pacman")) install.packages("pacman")
pacman::p_load("ggplot2", "googlesheets4", "dplyr", "officer","reshape2", "stringr", "flextable", "gridExtra", "ggpubr", "Rmisc", "rcompanion", "RColorBrewer", "googledrive", "gdata", "readxl", "DescTools","patchwork", "FSA", "rstatix", "tidyverse", "lme4", 'PMCRMplus', "EnvStats", "emmeans", "MuMIn", "sjstats", "lmerTest", "gargle", "FSA", "vegan")
install.packages("geoR")
```


# Importing Data
***
## We are downloading this dataset from our GoogleDrive folder. We will be using the package `googledrive`. Each GoogleDrive file has a unique ID that does not change throughout the lifespan of the document, even if the file name is changed. This ID is housed in the file's URL. 
Example: docs.google.com/spreadsheets/d/FILE_ID_GOES_HERE/other_information/. Below you will copy and paste that into the function `drive_download` within `as_id`. This will save the file locally in the specified path (in this case our data folder) and you will import the folder as you normally would. Downloading it this way decreases the potential human error when downloading, moving folders, renaming, saving etc and ensures that the most up to date file is being utilized. 

# Here we are importing data for our plug counts from google drive

```{r, plugLoadingData, include = TRUE}
# drive_auth <- function(email = gargle::gargle_oauth_email(),
#                        path = NULL,
#                        scopes = "https://www.googleapis.com/auth/drive",
#                        cache = gargle::gargle_oauth_cache(),
#                        use_oob = gargle::gargle_oob_default(),
#                        token = NULL) { ... }


 data <- drive_download(
 as_id("1xyLtNN61HYvXYIhD-B4Eca8BPCuEwTftA4936MfwM-w"),
   path = "../data/recruitDataset.xlsx", 
   overwrite = TRUE)

plugData <- read_excel("../data/recruitDataset.xlsx", sheet = 3)

#Changing columns to factors
plugData <- plugData %>% mutate_at(c('timepoint', 'rack', 'plug', 'color'), as.factor)

head(plugData)


```


#Adding Columns
***
I'm adding columns that compare the counts between the trained eye, untrained eye, and TagLab segmentation.

```{r, addColumns, include = TRUE}

#Adding columns that contain the absolute value of the difference between the trained eye counts and untrained eye counts, trained eye counts and taglab segmentation counts, and untrained eye counts and taglab segmentation counts
plugData1 <- plugData %>%
  select(timepoint, rack, plug, color, trainedEyeCount, untrainedEyeCount, taglabCount) %>% 
  group_by(timepoint, color, plug) %>% 
  mutate(
    trainedVsUntrainedAbs = abs(trainedEyeCount - untrainedEyeCount),
    trainedVsTaglabAbs = abs(trainedEyeCount - taglabCount), 
    untrainedVsTaglabAbs = abs(untrainedEyeCount - taglabCount ),
    trainedVsUntrained = (trainedEyeCount - untrainedEyeCount),
    trainedVsTaglab = (trainedEyeCount - taglabCount), 
    untrainedVsTaglab = (untrainedEyeCount - taglabCount)
  ) %>% 
  na.omit()


#Outputs the head of the data to see if it added correctly
head(plugData1)


```




# Analyzing data
***
## We are going to be determining if the counts from the untrained eye inventory and the Taglab segmentation inventory are significantly different from the trained eye counts using each plug as a replicate and between the three racks.


```{r, perPlugAccuracy, include = TRUE}
#Using the Welch two sample test to compare the trained eye count to the untrained eye count 
tVu <- plugData1 %>%
   select(timepoint, rack, plug, color, trainedEyeCount, untrainedEyeCount, taglabCount) %>%
   pivot_longer(cols = trainedEyeCount:taglabCount,
                names_to = "type",
                values_to = "counts") %>%
  mutate_at(c('type'), as.factor)

kruskal.test(counts ~ type, data = tVu)

#Using the Welch two sample test to compare the trained eye count to the taglab count 
tVu1 <- plugData1 %>%
   select(timepoint, rack, plug, color, trainedEyeCount, taglabCount) %>%
   pivot_longer(cols = trainedEyeCount:taglabCount,
                names_to = "type",
                values_to = "counts") %>%
  mutate_at(c('type'), as.factor)

t.test(counts ~ type, data = tVu1)


#Creating aovDataCounts to have the counts all be combined with their type as factors
aovDataCounts <- plugData1 %>% 
  select(timepoint, rack, plug, color, trainedEyeCount, untrainedEyeCount, taglabCount) %>% 
  pivot_longer(cols = trainedEyeCount:taglabCount,
               names_to = 'type', 
               values_to = 'counts') %>% 
mutate_at(c('type'), as.factor)

#testing the normality of our new counts column
# hist makes a histogram, shapiro-wilk test is a normality test
hist(aovDataCounts$counts)
shapiro.test(aovDataCounts$counts)


#trying to transform the data so it has a more normal distribution
hist(log(aovDataCounts$counts)+ 1)
shapiro.test(sqrt(aovDataCounts$counts))

#Trying boxcox transformation, it says im missing a lambda and there isnt a default
boxcoxTransform(aovDataCounts$counts)

#Anova test for the counts compared to type and color
anovaCounts <- aov(counts ~ type*color, data = aovDataCounts)
summary(anovaCounts)

kruskal.test(counts~type, data = aovDataCounts)


#Creating aovDataError which is the same layout as aovDataCounts but instead of the different counts we are looking at the error between trained eye counts, taglab, and untrained
aovDataError <- plugData1 %>% 
  select(timepoint, rack, plug, color, trainedVsUntrainedAbs, trainedVsTaglabAbs, untrainedVsTaglabAbs) %>% 
  pivot_longer(cols = trainedVsUntrainedAbs:untrainedVsTaglabAbs,
               names_to = 'type', 
               values_to = 'error') %>% 
mutate_at(c('type'), as.factor)



#Determining if the data is normal
# hist makes a histogram, shapiro-wilk test is a normality test
hist(aovDataError$error)
shapiro.test(aovDataError$error)

#Trying to normalize the data with log and log + 1 and sqrt
 hist(log(aovDataError$error))
 
 hist(log(aovDataError$error + 1))
 shapiro.test(sqrt(aovDataError$error))


#Running an anova on the error data by count type (trainedvstaglab) and color
anovaError <- aov(error ~ type*color, data = aovDataError)
summary(anovaError)

errorKW <- kruskal.test(error~type, data = aovDataError)

errorDunn <- dunnTest(x = aovDataError$error, g = aovDataError$type, method = 'bonferroni') 

adonis2(counts ~ type*timepoint*rack*color, data = aovDataCounts, permutations = 9999)





```
#Graphing
***
I am creating graphs to show what we found from our analysis results

```{r, graphing, include=TRUE}
#Adding the counts together by the type and by timepoint
# aovDataGraph <- aovDataCounts%>%
#    select(timepoint, rack, plug, color, trainedEyeCount, untrainedEyeCount, taglabCount) %>%
#    pivot_longer(cols = trainedEyeCount:taglabCount,
#                 names_to = "type",
#                 values_to = "counts") %>%
#   mutate_at(c('type'), as.factor)

#Adding the counts together by the type and by timepoint
rackCounts <- plugData1 %>% 
  group_by(timepoint, rack) %>%
  summarise(trainedEye = sum(trainedEyeCount)) %>% 
  summarise(untrainedEye = sum(untrainedEyeCount)) %>% 
  summarise(taglabCount = sum(taglabCount))


#Plotting the counts by type
dataCounts <- ggplot(aovDataCounts, aes(x = timepoint, y = counts)) + geom_bar(position = 'dodge', stat = 'identity', aes(fill = timepoint))+
  facet_grid(.~rack, scales = 'free')+ xlab("Timepoint (weeks)") + ylab("Time Taken (minutes)") +
    ggtitle("Scan Time Per Rack Per Timepoint") +
    theme_bw()

dataCounts
scanPlot <- scanPlot+theme(
    axis.text.x = element_text(size = 25, colour = "black", vjust = 0.5, hjust = 0.5, face= "bold"), 
    axis.title.x = element_text(size = 30, face = "bold"), 
    axis.title.y = element_text(size = 30, face = "bold"), 
    axis.text.y = element_text(colour = "black", size = 24, face = "bold"),
    legend.title = element_text(size = 30, face = "bold"), 
    legend.text = element_text(size = 25, face = "bold", colour = "black"), 
    panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour = "white"),
    panel.background = element_rect(fill = '#F5F5F5'), 
    plot.title = element_text(size = 37, face = "bold"), 
    axis.line = element_line(colour = "black"), 
    axis.ticks = element_line(color="black"), 
    text = element_text(size=40, color="black"), 
    legend.position = "right")
ggsave("../figures/scanTimePerTimepoint.png", plot = scanPlot, width = 15, height = 10, units = "in", dpi = 600)
scanPlot

```

